		###### MISSING DATA #######
There are three features with missing data (category 'unknown')
- JOB: a relatively small percentage of data (around 3%) 
- EDUCATION: a relatively small percentage of data (around 5%)
- CONTACT: a very high percentage of the data (around 40%)


		###### OUTLIER ANALYSIS #######
1/ Feature 'age'
The distribution is quite wide, with a small number of outliers. By zooming in the outlier region, we notice that the ratio between yes and no is much higher than in the center of the distribution.
This could indicate that samples that are outliers in this feature have an higher probability of saying yes. We will **not** remove the outliers from this feature

2/ Feature 'balance'
The distribution of the feature is very narrow, with outlier tails on both sides. 
The left outliers region is not particularly long, and it is mainly filled with 'no' samples -> we might remove the samples with balance lower than -1000. It also makes sense logically: a broke individual is not going to open a deposit.
The right region of outliers has two trends: in the first part (balance < 5000) we notice an increase of 'yes' samples, afterwards the outliers are mainly 'no'. 
In this case, we might keep only the samples with balance between -1000 and 5000

3/ Feature 'duration'
Once again the distribution is quite narrow, with a long outlier tail on the positive side.
Looking at the boxplot we notice that the distribution of 'yes' is centered around an higher value than the 'no'. This makes sense logically: if a customer accept an offer, the phone call will be longer - while if they are not interested, the call will be much shorted.
After the value 2000 (i.e. 33 minutes), however, the number of samples drastically reduces -> the samples in this area are not statistically meaningful, so we are going to remove them.

4/ Feature 'campaign'
Also in this case the distribution is narrow with a long positive tail.
There is a very limited number of 'yes' samples with campaign > 10, while the 'no's are much more. We can draw two conclusions from this:
a) we shall remove the outliers with campaign > 10
b) we shall advise our client to avoid calling the same individual more than 10 times


		###### CORRELATION ANALYSIS #######
1/ Categorical features (including 'day')
We observe some correlation between different categories and the probability of Yes. Hopefully, the model will identify and exploit such correlation
	JOB: the 'unknown' have an average probability of Yes - it could be a mix of the other categories
	EDUCATION: the 'unknown' have less than average probability of Yes, between the probability of class 'secondary' and 'primary' - could be a mix of the two categories
	CONTACT: the 'unknown' have a much less than average probability of Yes - we must keep the class as is

2/ Feature 'age'
We notice that in the center of the distribution, the probability of Yes is more or less constant -> we can consider to group together some values to reduce the number of possible values. 
In the outlier region, on the other hand, we observe and increase in the probability of Yes. However, after the value 80 the probability is very scattered -> the number of samples in this region is extremely low, we might consider to cut them off

3/ Feature 'campaign'
In the outlier analysis, we suggested to remove the samples with campaign > 10. Looking at the correlation, we notice that the probability of Yes drops after 10, with some peaks that are due to the very limited number of samples. 
In conclusion, the correlation analysis corroborates the conclusion of the outliers analysis

4/ Feature 'balance'
Once again, in the regions identified as outliers the probability of Yes is very low, with some peaks due to the low number of samples.
However, we notice that this behavior starts slightly after the region previously identified. Hence, we might consider 10000 instead of 5000 as upper bound.

5/ Feature 'duration'
We observe that the probability of Yes increases with the increase of the duration. However, once again, after some point the trend disrupts because of the limited number of samples.
Hence, also this analysis confirms that we shall remove the outliers with value above 2000


		############################################################
		#############		   XGBOOST		############
		############################################################
Training evaluation: train - test split 70-30 and metrics
	train confusion matrix
	test confusion matrix
	train accuracy
	test accuracy
	train recall
	test recall
	train AUC
	test AUC score

Final evaluation: mean accuracy with 5-fold cross validation
Find target categories with Partial Dependence Plot


		########## DEFAULT DATASET - DEFAULT PARAMS     ############
TRAINING EVALUATION
Train confusion matrix:
[[25876   108]
 [  529  1487]]
Test confusion matrix:
[[10864   256]
 [  550   330]]
Train accuracy score:  0.97725
Test accuracy score:  0.9328333333333333
Train recall score:  0.7375992063492064
Test recall score:  0.375
Train AUC score:  0.86672140120416
Test AUC score:  0.6759892086330935

FINAL EVALUATION
Train accuracy score:  0.977375
Test accuracy score:  0.6129999999999999
Train recall score:  0.7519877111654266
Test recall score:  0.19870585432672266

The train accuracy is higher than the test one -> we are overfitting
The recall is extremely low: this is due to the labels imbalance. 
Let's try to improve performance by using the parameter scale_pos_weight


		########## DEFAULT DATASET - SCALE_POS_WEIGHT   ############
Used scale_pos_weight parameter with the suggested value = sum(negative instances) / sum(positive instances) = 12.8

TRAIN EVALUATION
Train confusion matrix:
[[24449  1535]
 [    7  2009]]
Test confusion matrix:
[[10188   932]
 [  221   659]]
Train accuracy score:  0.9449285714285715
Test accuracy score:  0.9039166666666667
Train recall score:  0.9965277777777778
Test recall score:  0.7488636363636364
Train AUC score:  0.9687264812534209
Test AUC score:  0.8325253433616744

The introduction of the parameter scale_pos_weight increased the recall performance, at the cost of lower accuracy.
However, we are still far from the goal.


		########## DEFAULT DATASET - INITIAL TUNING   ############
1/ ITERATION 1
min_child_weight=10,
max_depth=4,
n_estimators=200,
scale_pos_weight=spw

Train confusion matrix:
[[23749  2235]
 [   29  1987]]
Test confusion matrix:
[[10035  1085]
 [  167   713]]
Train accuracy score:  0.9191428571428572
Test accuracy score:  0.8956666666666667
Train recall score:  0.9856150793650794
Test recall score:  0.8102272727272727
Train AUC score:  0.9498003044608648
Test AUC score:  0.8563276651406146


2/ ITERATION 2
min_child_weight=20,
max_depth=4,
n_estimators=400,
scale_pos_weight=spw

Train confusion matrix:
[[24055  1929]
 [    7  2009]]
Test confusion matrix:
[[10146   974]
 [  198   682]]
Train accuracy score:  0.9308571428571428
Test accuracy score:  0.9023333333333333
Train recall score:  0.9965277777777778
Test recall score:  0.775
Train AUC score:  0.9611448925834701
Test AUC score:  0.843705035971223


3/ ITERATION 3
min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=20

Train confusion matrix:
[[23656  2328]
 [    3  2013]]
Test confusion matrix:
[[9989 1131]
 [ 155  725]]
Train accuracy score:  0.91675
Test accuracy score:  0.8928333333333334
Train recall score:  0.9985119047619048
Test recall score:  0.8238636363636364
Train AUC score:  0.9544591543513956
Test AUC score:  0.8610775016350555


4/ ITERATION 4
min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=50

Train confusion matrix:
[[23012  2972]
 [    0  2016]]
Test confusion matrix:
[[9716 1404]
 [ 112  768]]
Train accuracy score:  0.8938571428571429
Test accuracy score:  0.8736666666666667
Train recall score:  1.0
Test recall score:  0.8727272727272727
Train AUC score:  0.942810960591133
Test AUC score:  0.8732341399607586

Train accuracy score:  0.9071999999999999
Test accuracy score:  0.5257499999999999
Train recall score:  1.0
Test recall score:  0.3150866535644095

Good improvement on the recall, however it overfits the train positive samples


5/ ITERATION 5
min_child_weight=10,
max_depth=4,
n_estimators=500,
scale_pos_weight=40

Train confusion matrix:
[[23931  2053]
 [    0  2016]]
Test confusion matrix:
[[10014  1106]
 [  183   697]]
Train accuracy score:  0.9266785714285715
Test accuracy score:  0.8925833333333333
Train recall score:  1.0
Test recall score:  0.7920454545454545
Train AUC score:  0.960494919950739
Test AUC score:  0.8462925114453892


6/ ITERATION 6
min_child_weight=15,
max_depth=4,
n_estimators=200,
scale_pos_weight=40

Train confusion matrix:
[[22587  3397]
 [    0  2016]]
Test confusion matrix:
[[9556 1564]
 [  95  785]]
Train accuracy score:  0.8786785714285714
Test accuracy score:  0.86175
Train recall score:  1.0
Test recall score:  0.8920454545454546
Train AUC score:  0.9346328509852216
Test AUC score:  0.8756989862655331

Good predictions on the positive, but we lost slightly too much in the negative

7/ ITERATION 7
min_child_weight=15,
max_depth=5,
n_estimators=300,
scale_pos_weight=40

Train confusion matrix:
[[23798  2186]
 [    0  2016]]
Test confusion matrix:
[[9977 1143]
 [ 166  714]]
Train accuracy score:  0.9219285714285714
Test accuracy score:  0.8909166666666667
Train recall score:  1.0
Test recall score:  0.8113636363636364
Train AUC score:  0.9579356527093595
Test AUC score:  0.8542879332897318

8/ ITERATION 8
min_child_weight=15,
max_depth=5,
n_estimators=200,
scale_pos_weight=40


Train confusion matrix:
[[23292  2692]
 [    0  2016]]
Test confusion matrix:
[[9784 1336]
 [ 130  750]]
Train accuracy score:  0.9038571428571428
Test accuracy score:  0.8778333333333334
Train recall score:  1.0
Test recall score:  0.8522727272727273
Train AUC score:  0.9481988916256158
Test AUC score:  0.8660644211903203

9/ITERATION 9
min_child_weight=10,
max_depth=5,
n_estimators=200,
scale_pos_weight=40

Train confusion matrix:
[[23450  2534]
 [    0  2016]]
Test confusion matrix:
[[9862 1258]
 [ 134  746]]
Train accuracy score:  0.9095
Test accuracy score:  0.884
Train recall score:  1.0
Test recall score:  0.8477272727272728
Train AUC score:  0.951239224137931
Test AUC score:  0.8672988881621975


10/ ITERATION 10
min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=40

Train confusion matrix:
[[23058  2926]
 [    0  2016]]
Test confusion matrix:
[[9735 1385]
 [ 123  757]]
Train accuracy score:  0.8955
Test accuracy score:  0.8743333333333333
Train recall score:  1.0
Test recall score:  0.8602272727272727
Train AUC score:  0.9436961206896552
Test AUC score:  0.8678384565075213

11/ ITERATION 11	
Add the learning_rate as tuned parameter as well:
min_child_weight=15,
max_depth=4,
n_estimators=500,
learning_rate=0.1,
scale_pos_weight=50

Train confusion matrix:
[[21992  3992]
 [    2  2014]]
Test confusion matrix:
[[9338 1782]
 [  61  819]]
Train accuracy score:  0.8573571428571428
Test accuracy score:  0.8464166666666667
Train recall score:  0.9990079365079365
Test recall score:  0.9306818181818182
Train AUC score:  0.9226874657909141
Test AUC score:  0.8852150098103335

We improved a lot on the positive results, but we are slightly underfitting -> increase the number of trees

12/ ITERATION 12
min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=50

Train confusion matrix:
[[22948  3036]
 [    0  2016]]
Test confusion matrix:
[[9686 1434]
 [ 102  778]]
Train accuracy score:  0.8915714285714286
Test accuracy score:  0.872
Train recall score:  1.0
Test recall score:  0.884090909090909
Train AUC score:  0.9415794334975369
Test AUC score:  0.8775670372792674

Train accuracy score:  0.907175
Test accuracy score:  0.490925
Train recall score:  1.0
Test recall score:  0.3147257459353225



CONCLUSIONS -> ITERATIONS 4 AND 12 ARE MORE OR LESS EQUIVALENT
However, iteration 12 has more trees, which suits better for early stopping -> we fix the parameters to iteration 12
min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=50,
random_state=42


		########## UNDERSAMPLED DATASET - ITERATION 4 AND 12 TUNING  ############
Undersampling with strategy 1.0

min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=50

Train confusion matrix:
[[1535  481]
 [   0 2016]]
Test confusion matrix:
[[8017 3103]
 [  21  859]]
Train accuracy score:  0.8807043650793651
Test accuracy score:  0.7396666666666667
Train recall score:  1.0
Test recall score:  0.9761363636363637
Train AUC score:  0.880704365079365
Test AUC score:  0.8485448005232178

Now the training set is balanced, hence we should reduce the pos_weight. 

min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=spw

Train confusion matrix:
[[1645  371]
 [   0 2016]]
Test confusion matrix:
[[8420 2700]
 [  27  853]]
Train accuracy score:  0.9079861111111112
Test accuracy score:  0.77275
Train recall score:  1.0
Test recall score:  0.9693181818181819
Train AUC score:  0.9079861111111112
Test AUC score:  0.8632562132112491

Better fit (accuracy) without loosing a lot in the recall -> good!
To increase the test accuracy, we could reduce a bit the ratio of the undersampler -> more examples of Negative samples for the model to train on
Strategy 0.3

min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=spw

Train confusion matrix:
[[5826  894]
 [   0 2016]]
Test confusion matrix:
[[9342 1778]
 [  81  799]]
Train accuracy score:  0.8976648351648352
Test accuracy score:  0.8450833333333333
Train recall score:  1.0
Test recall score:  0.9079545454545455
Train AUC score:  0.9334821428571429
Test AUC score:  0.8740312295618051

Good result, but ITERATION 4 is still better in term of accuracy -> try to further lower the undersampling ratio
strategy 0.2

min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=spw

Train confusion matrix:
[[8955 1125]
 [   1 2015]]
Test confusion matrix:
[[9670 1450]
 [ 101  779]]
Train accuracy score:  0.9069113756613757
Test accuracy score:  0.87075
Train recall score:  0.9995039682539683
Test recall score:  0.8852272727272728
Train AUC score:  0.9439484126984127
Test AUC score:  0.8774157946370177

LET'S NOW TRY OUT WITH ITERATION 12 TUNING
min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=50

Train confusion matrix:
[[8501 1579]
 [   0 2016]]
Test confusion matrix:
[[9229 1891]
 [  56  824]]
Train accuracy score:  0.8694609788359788
Test accuracy score:  0.83775
Train recall score:  1.0
Test recall score:  0.9363636363636364
Train AUC score:  0.9216765873015873
Test AUC score:  0.883154839764552

Good resutls, almost comparable to the ones above.
But this time we are weighting a lot the postive samples -> reduce it

min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=spw

Train confusion matrix:
[[8948 1132]
 [   2 2014]]
Test confusion matrix:
[[9646 1474]
 [  98  782]]
Train accuracy score:  0.90625
Test accuracy score:  0.869
Train recall score:  0.9990079365079365
Test recall score:  0.8886363636363637
Train AUC score:  0.9433531746031746
Test AUC score:  0.8780412034009156

Train accuracy score:  0.907175
Test accuracy score:  0.490925
Train recall score:  1.0
Test recall score:  0.3147257459353225

Performance very similar to iteration 4.

Conclusion -> using a more balanced training set increase performance (if we reduce the scale pos weight)
However, we observe an underfitting due to the small portion of the used training set


		########## OVERSAMPLED DATASET - ITERATION 12 AND 4 TUNING  ############
strategy 1.0
The training set is much bigger than in the undersampled case -> a lot of copies of the positive samples
The model should highly overfit the positive training samples

min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=spw

Train confusion matrix:
[[23896  2088]
 [   17 25967]]
Test confusion matrix:
[[10081  1039]
 [  180   700]]
Train accuracy score:  0.9594943041871922
Test accuracy score:  0.8984166666666666
Train recall score:  0.9993457512315271
Test recall score:  0.7954545454545454
Train AUC score:  0.9594943041871921
Test AUC score:  0.8510096468279921


min_child_weight=15,
max_depth=4,
n_estimators=300,
learning_rate=0.3,
scale_pos_weight=spw

Train confusion matrix:
[[23916  2068]
 [   12 25972]]
Test confusion matrix:
[[10074  1046]
 [  182   698]]
Train accuracy score:  0.9599753694581281
Test accuracy score:  0.8976666666666666
Train recall score:  0.9995381773399015
Test recall score:  0.7931818181818182
Train AUC score:  0.9599753694581281
Test AUC score:  0.8495585349901896

We are overfitting the positive train samples, because a lot of copies are present in the training set.
Once again, by reducing the sampling strategy we shuold improve the performance

strategy 0.2

min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=spw

Train confusion matrix:
[[23935  2049]
 [   13  5183]]
Test confusion matrix:
[[10091  1029]
 [  180   700]]
Train accuracy score:  0.9338678640153945
Test accuracy score:  0.89925
Train recall score:  0.9974980754426482
Test recall score:  0.7954545454545454
Train AUC score:  0.9593209281154129
Test AUC score:  0.8514592871157619


min_child_weight=15,
max_depth=4,
n_estimators=300,
learning_rate=0.3,
scale_pos_weight=spw

Train confusion matrix:
[[23915  2069]
 [   13  5183]]
Test confusion matrix:
[[10106  1014]
 [  177   703]]
Train accuracy score:  0.9332264271969211
Test accuracy score:  0.90075
Train recall score:  0.9974980754426482
Test recall score:  0.7988636363636363
Train AUC score:  0.958936075898664
Test AUC score:  0.8538382930019621

Not a significant improve. 

CONCLUSIONS 
Undersampling: training set size 28'000 -> 12'000
Oversampling: training set size 28'000 -> 31'000

Since we are more interested in the Positive samples, which are the minority class, undersampling is more advisable:
- The number of "original" positive samples is always the same
- But the number of negative samples the model trains on is significantly less 
Hence, the model will learn better the Positive samples, without using too many resources on the negative ones 

HOWEVER
Under and Over sampling are based on the assumption that the sampled data do not represent the real data distribution.
From the problem description, however, we deduce that the strong unbalance is expected.
Hence, under and over sampling are not the recommended way to proceed


		########## MISSING DATA JOB - ITERATION 4 AND 12 TUNING  ############
Iteration 4 tuning
min_child_weight=15,
max_depth=4,
n_estimators=300,
learning_rate=0.3,
scale_pos_weight=50

Train confusion matrix:
[[22850  2958]
 [    0  2027]]
Test confusion matrix:
[[9655 1423]
 [ 106  746]]
Train accuracy score:  0.893730914316508
Test accuracy score:  0.8718357082984074
Train recall score:  1.0
Test recall score:  0.8755868544600939
Train AUC score:  0.9426921884686918
Test AUC score:  0.8735670325739718

Minimal improvement in the test recall, but we are talking about 0.1% order of magnitude.
It is expected, as the number of samples with missing job is very small


Iteration 12 tuning
min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=50

Train confusion matrix:
[[22736  3072]
 [    0  2027]]
Test confusion matrix:
[[9593 1485]
 [ 102  750]]
Train accuracy score:  0.8896353511765762
Test accuracy score:  0.8669740150880134
Train recall score:  1.0
Test recall score:  0.8802816901408451
Train AUC score:  0.9404835709857409
Test AUC score:  0.8731161113639773

Minimal decrease in the results (once again, the difference is in the order of 0.1%)

====> we can either leave the category or drop the rows, the effect is very small either ways


		########## MISSING DATA EDUCATION - ITERATION 4 AND 12 TUNING  ############
1/ Dropped the rows with missing data
Iteration 4 tuning
min_child_weight=15,
max_depth=4,
n_estimators=300,
learning_rate=0.3,
scale_pos_weight=50

Train confusion matrix:
[[22035  2933]
 [    0  1960]]
Test confusion matrix:
[[9352 1349]
 [  88  752]]
Train accuracy score:  0.8910799168152109
Test accuracy score:  0.8754873927735898
Train recall score:  1.0
Test recall score:  0.8952380952380953
Train AUC score:  0.9412648189682794
Test AUC score:  0.8845875552351582

Slight improvement in the accuracy, 2% improvement in the Recall


Iteration 12 tuning
min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=50

Train confusion matrix:
[[22123  2845]
 [    0  1960]]
Test confusion matrix:
[[9401 1300]
 [  88  752]]
Train accuracy score:  0.8943478906714201
Test accuracy score:  0.8797331253790832
Train recall score:  1.0
Test recall score:  0.8952380952380953
Train AUC score:  0.9430270746555591
Test AUC score:  0.8868770608888354

Slight improvement in the accuracy, 1% improvement in the Recall
=====> overall, removing the missing data seems to improve performance

2/ Substitute with most frequent value (=secondary)
Iteration 4 tuning
min_child_weight=15,
max_depth=4,
n_estimators=300,
learning_rate=0.3,
scale_pos_weight=50

Train confusion matrix:
[[22912  3072]
 [    0  2016]]
Test confusion matrix:
[[9701 1419]
 [ 104  776]]
Train accuracy score:  0.8902857142857142
Test accuracy score:  0.8730833333333333
Train recall score:  1.0
Test recall score:  0.8818181818181818
Train AUC score:  0.9408866995073892
Test AUC score:  0.8771051340745586

Slight decreace in all metrics, exept a small improvement in the test recall.
However, dropping the missing values performs better than substituing them 


Iteration 12 tuning
min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=50

Train confusion matrix:
[[22967  3017]
 [    0  2016]]
Test confusion matrix:
[[9706 1414]
 [ 105  775]]
Train accuracy score:  0.89225
Test accuracy score:  0.8734166666666666
Train recall score:  1.0
Test recall score:  0.8806818181818182
Train AUC score:  0.9419450431034483
Test AUC score:  0.8767617724002618

Slight decreace in all metrics, exept a small improvement in the test recall.
However, dropping the missing values performs better than substituing them.

======> we decide to DROP the rows with missing values


		########## DROP MISSING JOB & EDUCATION - ITERATION 4 AND 12 TUNING  ############
Iteration 4 tuning
min_child_weight=15,
max_depth=4,
n_estimators=300,
learning_rate=0.3,
scale_pos_weight=50

Train confusion matrix:
[[21883  2995]
 [    0  1958]]
Test confusion matrix:
[[9246 1422]
 [  80  754]]
Train accuracy score:  0.8883961842301387
Test accuracy score:  0.869414014953921
Train recall score:  1.0
Test recall score:  0.9040767386091128
Train AUC score:  0.9398062545220677
Test AUC score:  0.8853904502944326

Improved the recall (1%) at the cost of accuracy (1%) -> we could balance this trade off by changing a bit the scale_pos_weight


Iteration 12 tuning
min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=50

Train confusion matrix:
[[21884  2994]
 [    0  1958]]
Test confusion matrix:
[[9232 1436]
 [  77  757]]
Train accuracy score:  0.8884334476076912
Test accuracy score:  0.8684576595374718
Train recall score:  1.0
Test recall score:  0.907673860911271
Train AUC score:  0.9398263526006914
Test AUC score:  0.8865328434665092

Improved the recall (1%) at the cost of accuracy (1%) -> we could balance this trade off by changing a bit the scale_pos_weight


New tuning: reduced the scale_pos_weight
Iteration 4 tuning
min_child_weight=15,
max_depth=4,
n_estimators=300,
learning_rate=0.3,
scale_pos_weight=30

Train confusion matrix:
[[22275  2603]
 [    0  1958]]
Test confusion matrix:
[[9416 1252]
 [  89  745]]
Train accuracy score:  0.9030034282307349
Test accuracy score:  0.8834115805946792
Train recall score:  1.0
Test recall score:  0.8932853717026379
Train AUC score:  0.9476847013425517
Test AUC score:  0.8879625208719414

Performance comparable to the one obtained dropping the education only and prior tuning


Iteration 12 tuning
min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=30

Train confusion matrix:
[[22301  2577]
 [    0  1958]]
Test confusion matrix:
[[9416 1252]
 [  88  746]]
Train accuracy score:  0.903972276047101
Test accuracy score:  0.8834985219961746
Train recall score:  1.0
Test recall score:  0.894484412470024
Train AUC score:  0.9482072513867674
Test AUC score:  0.8885620412556344

Performance slightly better but definitely comparable to the one obtained dropping the education only and prior tuning



		###### NEXT STEPS #######
- for binary features, only one of the two columns of one hot encoding is used -> change encoding to make those features binary (and "save" one column)
- Think what to do with outliers


		###### SUMMARY #####
Over and under sampling might help improve the performance (especially the latter).
However, they are based on the assumption that the available samples do not represent the trues data distribution.
Since this is not the case (most likely), the two approaches are not optimal.

===> The only way we are going to deal with the unabalance is via SCALE_POS_WEIGHT

MISSING DATA
There are three features with missing data: job, education and contact.
In the latter, the missing data are almost 40% -> we must leave the category 'unknown'
The missing data in job are very few (around 0.6%), while education is a bit more (around 3.8%)
For job, we either leave the extra category, or drop the rows, while for education we can also fill in the most frequent feature (prob of Y is comparable)
We expect little to no effect if we remove the missing values from the former, while the impact should be greater for the latter.

- Removing the missing values in feature 'Job' has a non-noticeable effect on the performance
- Removing the missing values in feature 'Education' increase accuracy (1%) and recall (2%)
- Substituting the missing values in feature 'Education' slightly improve the performance, but less than dropping the rows
- Dropping the rows in both Job and Education improve the performance, both in terms of accuracy and recall - but we need to decrease the scale_pos_weight value from 50 to 30

===> We decide to drop the rows with missing values

Regarding the feature contact, we might consider to drop the whole feature as there are a lot of missing values.
However, looking at the feature importance, the feature 'contact - unknown' is the one with the highest value!
Which means that the most effective way for the model to separate the data is based on the contact unknown.




To know which features influence the Y/N 
- partial dependent plot
- predicted vs observed -> like the frequency plot + scatter, but the scatter is both labels and prediction

Look if it's possible to group categories together.

Check feature importance: one way to do it is create a random feature and if a feature importance is lower than the random, just discard it
