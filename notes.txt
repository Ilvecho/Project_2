XGBoost
Evaluation: mean accuracy with 5-fold cross validation

1/ No interventions on the dataset; default parameters
Train accuracy score:  0.977375
Test accuracy score:  0.6129999999999999

Train recall score:  0.7519877111654266
Test recall score:  0.19870585432672266

The recall is extremely low: this is due to the labels imbalance. 
Let's try to improve performance by using the parameter scale_pos_weight

2/ No interventions on the dataset; used scale_pos_weight parameter, with the suggested value sum(negative instances) / sum(positive instances)
Train accuracy score:  0.9456562500000001
Test accuracy score:  0.5164500000000001

Train recall score:  0.9947344214833217
Test recall score:  0.24532606753618008

The introduction of the parameter scale_pos_weight increased the recall performance, at the cost of lower accuracy.
However, we are still far from the goal.


Call Notes:
for plotting use plotly instead of matplotlib 


We are overfitting -> we need to prune the trees:
- min_child_weight
- max_depth
- increase n_trees to 1000

To know which features influence the Y/N 
- partial dependent plot
- predicted vs observed -> like the frequency plot + scatter, but the scatter is both labels and prediction

Look if it's possible to group categories together.
Check feature importance:
one way to do it is create a random feature and if a feature importance is lower than the random, just discard it

Next steps:
- Reasonable tuning with above 3 params
- Feature engineering
- lastly, fine tuning 

