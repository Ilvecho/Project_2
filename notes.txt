		###### MISSING DATA #######
There are three features with missing data (category 'unknown')
- JOB: a relatively small percentage of data (around 3%) 
- EDUCATION: a relatively small percentage of data (around 5%)
- CONTACT: a very high percentage of the data (around 40%)


		###### OUTLIER ANALYSIS #######
1/ Feature 'age'
The distribution is quite wide, with a small number of outliers. By zooming in the outlier region, we notice that the ratio between yes and no is much higher than in the center of the distribution.
This could indicate that samples that are outliers in this feature have an higher probability of saying yes. We will **not** remove the outliers from this feature

2/ Feature 'balance'
The distribution of the feature is very narrow, with outlier tails on both sides. 
The left outliers region is not particularly long, and it is mainly filled with 'no' samples -> we might remove the samples with balance lower than -1000. It also makes sense logically: a broke individual is not going to open a deposit.
The right region of outliers has two trends: in the first part (balance < 5000) we notice an increase of 'yes' samples, afterwards the outliers are mainly 'no'. 
In this case, we might keep only the samples with balance between -1000 and 5000

3/ Feature 'duration'
Once again the distribution is quite narrow, with a long outlier tail on the positive side.
Looking at the boxplot we notice that the distribution of 'yes' is centered around an higher value than the 'no'. This makes sense logically: if a customer accept an offer, the phone call will be longer - while if they are not interested, the call will be much shorted.
After the value 2000 (i.e. 33 minutes), however, the number of samples drastically reduces -> the samples in this area are not statistically meaningful, so we are going to remove them.

4/ Feature 'campaign'
Also in this case the distribution is narrow with a long positive tail.
There is a very limited number of 'yes' samples with campaign > 10, while the 'no's are much more. We can draw two conclusions from this:
a) we shall remove the outliers with campaign > 10
b) we shall advise our client to avoid calling the same individual more than 10 times


		###### CORRELATION ANALYSIS #######
1/ Categorical features (including 'day')
We observe some correlation between different categories and the probability of Yes. Hopefully, the model will identify and exploit such correlation
	JOB: the 'unknown' have an average probability of Yes - it could be a mix of the other categories
	EDUCATION: the 'unknown' have less than average probability of Yes, between the probability of class 'secondary' and 'primary' - could be a mix of the two categories
	CONTACT: the 'unknown' have a much less than average probability of Yes - we must keep the class as is

2/ Feature 'age'
We notice that in the center of the distribution, the probability of Yes is more or less constant -> we can consider to group together some values to reduce the number of possible values. 
In the outlier region, on the other hand, we observe and increase in the probability of Yes. However, after the value 80 the probability is very scattered -> the number of samples in this region is extremely low, we might consider to cut them off

3/ Feature 'campaign'
In the outlier analysis, we suggested to remove the samples with campaign > 10. Looking at the correlation, we notice that the probability of Yes drops after 10, with some peaks that are due to the very limited number of samples. 
In conclusion, the correlation analysis corroborates the conclusion of the outliers analysis

4/ Feature 'balance'
Once again, in the regions identified as outliers the probability of Yes is very low, with some peaks due to the low number of samples.
However, we notice that this behavior starts slightly after the region previously identified. Hence, we might consider 10000 instead of 5000 as upper bound.

5/ Feature 'duration'
We observe that the probability of Yes increases with the increase of the duration. However, once again, after some point the trend disrupts because of the limited number of samples.
Hence, also this analysis confirms that we shall remove the outliers with value above 2000


		############################################################
		#############		   XGBOOST		############
		############################################################
Training evaluation: train - test split 70-30 and metrics
	train confusion matrix
	test confusion matrix
	train accuracy
	test accuracy
	train recall
	test recall
	train AUC
	test AUC score

Final evaluation: mean accuracy with 5-fold cross validation
Find target categories with Partial Dependence Plot


		########## DEFAULT DATASET - DEFAULT PARAMS     ############
TRAINING EVALUATION
Train confusion matrix:
[[25876   108]
 [  529  1487]]
Test confusion matrix:
[[10864   256]
 [  550   330]]
Train accuracy score:  0.97725
Test accuracy score:  0.9328333333333333
Train recall score:  0.7375992063492064
Test recall score:  0.375
Train AUC score:  0.86672140120416
Test AUC score:  0.6759892086330935

FINAL EVALUATION
Train accuracy score:  0.977375
Test accuracy score:  0.6129999999999999
Train recall score:  0.7519877111654266
Test recall score:  0.19870585432672266

The train accuracy is higher than the test one -> we are overfitting
The recall is extremely low: this is due to the labels imbalance. 
Let's try to improve performance by using the parameter scale_pos_weight


		########## DEFAULT DATASET - SCALE_POS_WEIGHT   ############
Used scale_pos_weight parameter with the suggested value = sum(negative instances) / sum(positive instances) = 12.8

TRAIN EVALUATION
Train confusion matrix:
[[24449  1535]
 [    7  2009]]
Test confusion matrix:
[[10188   932]
 [  221   659]]
Train accuracy score:  0.9449285714285715
Test accuracy score:  0.9039166666666667
Train recall score:  0.9965277777777778
Test recall score:  0.7488636363636364
Train AUC score:  0.9687264812534209
Test AUC score:  0.8325253433616744

The introduction of the parameter scale_pos_weight increased the recall performance, at the cost of lower accuracy.
However, we are still far from the goal.


		########## DEFAULT DATASET - INITIAL TUNING   ############
1/ ITERATION 1
min_child_weight=10,
max_depth=4,
n_estimators=200,
scale_pos_weight=spw

Train confusion matrix:
[[23749  2235]
 [   29  1987]]
Test confusion matrix:
[[10035  1085]
 [  167   713]]
Train accuracy score:  0.9191428571428572
Test accuracy score:  0.8956666666666667
Train recall score:  0.9856150793650794
Test recall score:  0.8102272727272727
Train AUC score:  0.9498003044608648
Test AUC score:  0.8563276651406146


2/ ITERATION 2
min_child_weight=20,
max_depth=4,
n_estimators=400,
scale_pos_weight=spw

Train confusion matrix:
[[24055  1929]
 [    7  2009]]
Test confusion matrix:
[[10146   974]
 [  198   682]]
Train accuracy score:  0.9308571428571428
Test accuracy score:  0.9023333333333333
Train recall score:  0.9965277777777778
Test recall score:  0.775
Train AUC score:  0.9611448925834701
Test AUC score:  0.843705035971223


3/ ITERATION 3
min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=20

Train confusion matrix:
[[23656  2328]
 [    3  2013]]
Test confusion matrix:
[[9989 1131]
 [ 155  725]]
Train accuracy score:  0.91675
Test accuracy score:  0.8928333333333334
Train recall score:  0.9985119047619048
Test recall score:  0.8238636363636364
Train AUC score:  0.9544591543513956
Test AUC score:  0.8610775016350555


4/ ITERATION 4
min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=50

Train confusion matrix:
[[23012  2972]
 [    0  2016]]
Test confusion matrix:
[[9716 1404]
 [ 112  768]]
Train accuracy score:  0.8938571428571429
Test accuracy score:  0.8736666666666667
Train recall score:  1.0
Test recall score:  0.8727272727272727
Train AUC score:  0.942810960591133
Test AUC score:  0.8732341399607586

Train accuracy score:  0.9071999999999999
Test accuracy score:  0.5257499999999999
Train recall score:  1.0
Test recall score:  0.3150866535644095

Good improvement on the recall, however it overfits the train positive samples



		###### NEXT STEPS #######
- Basic model & data
- a bit of tuning with the three params:
	min_child_weight
	max_depth
	n_estimators -> you can go up to 100

- Unbalanced dataset: Use "imbalanced learn library"
	UNDERSAMPLER to perform undersampling -> might overfit the data, because it is generated from too little set
	SNOTE to perform oversampling -> might overfit the positive, as there will be a lot of "copies" in the training
- Decide what to do with the outliers:
	JOB: leave the category, or remove the rows
	EDUCATION: leave the category, make it the most frequent, or remove the rows
	CONTACT: leave the category
- Think what to do with outliers






To know which features influence the Y/N 
- partial dependent plot
- predicted vs observed -> like the frequency plot + scatter, but the scatter is both labels and prediction

Look if it's possible to group categories together.

Check feature importance: one way to do it is create a random feature and if a feature importance is lower than the random, just discard it
