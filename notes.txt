		###### MISSING DATA #######
There are three features with missing data (category 'unknown')
- JOB: a relatively small percentage of data (around 3%) 
- EDUCATION: a relatively small percentage of data (around 5%)
- CONTACT: a very high percentage of the data (around 40%)


		###### OUTLIER ANALYSIS #######
1/ Feature 'age'
The distribution is quite wide, with a small number of outliers. By zooming in the outlier region, we notice that the ratio between yes and no is much higher than in the center of the distribution.
This could indicate that samples that are outliers in this feature have an higher probability of saying yes. We will **not** remove the outliers from this feature

2/ Feature 'balance'
The distribution of the feature is very narrow, with outlier tails on both sides. 
The left outliers region is not particularly long, and it is mainly filled with 'no' samples -> we might remove the samples with balance lower than -1000. It also makes sense logically: a broke individual is not going to open a deposit.
The right region of outliers has two trends: in the first part (balance < 5000) we notice an increase of 'yes' samples, afterwards the outliers are mainly 'no'. 
In this case, we might keep only the samples with balance between -1000 and 5000

3/ Feature 'duration'
Once again the distribution is quite narrow, with a long outlier tail on the positive side.
Looking at the boxplot we notice that the distribution of 'yes' is centered around an higher value than the 'no'. This makes sense logically: if a customer accept an offer, the phone call will be longer - while if they are not interested, the call will be much shorted.
After the value 2000 (i.e. 33 minutes), however, the number of samples drastically reduces -> the samples in this area are not statistically meaningful, so we are going to remove them.

4/ Feature 'campaign'
Also in this case the distribution is narrow with a long positive tail.
There is a very limited number of 'yes' samples with campaign > 10, while the 'no's are much more. We can draw two conclusions from this:
a) we shall remove the outliers with campaign > 10
b) we shall advise our client to avoid calling the same individual more than 10 times


		###### CORRELATION ANALYSIS #######
1/ Categorical features (including 'day')
We observe some correlation between different categories and the probability of Yes. Hopefully, the model will identify and exploit such correlation
	JOB: the 'unknown' have an average probability of Yes - it could be a mix of the other categories
	EDUCATION: the 'unknown' have less than average probability of Yes, between the probability of class 'secondary' and 'primary' - could be a mix of the two categories
	CONTACT: the 'unknown' have a much less than average probability of Yes - we must keep the class as is

2/ Feature 'age'
We notice that in the center of the distribution, the probability of Yes is more or less constant -> we can consider to group together some values to reduce the number of possible values. 
In the outlier region, on the other hand, we observe and increase in the probability of Yes. However, after the value 80 the probability is very scattered -> the number of samples in this region is extremely low, we might consider to cut them off

3/ Feature 'campaign'
In the outlier analysis, we suggested to remove the samples with campaign > 10. Looking at the correlation, we notice that the probability of Yes drops after 10, with some peaks that are due to the very limited number of samples. 
In conclusion, the correlation analysis corroborates the conclusion of the outliers analysis

4/ Feature 'balance'
Once again, in the regions identified as outliers the probability of Yes is very low, with some peaks due to the low number of samples.
However, we notice that this behavior starts slightly after the region previously identified. Hence, we might consider 10000 instead of 5000 as upper bound.

5/ Feature 'duration'
We observe that the probability of Yes increases with the increase of the duration. However, once again, after some point the trend disrupts because of the limited number of samples.
Hence, also this analysis confirms that we shall remove the outliers with value above 2000


		############################################################
		#############		   XGBOOST		############
		############################################################
Training evaluation: train - test split 70-30 and metrics
	train confusion matrix
	test confusion matrix
	train accuracy
	test accuracy
	train recall
	test recall
	train AUC
	test AUC score

Final evaluation: mean accuracy with 5-fold cross validation


		########## DEFAULT DATASET - DEFAULT PARAMS     ############

	********

The recall is extremely low: this is due to the labels imbalance. 
Let's try to improve performance by using the parameter scale_pos_weight


		########## DEFAULT DATASET - SCALE_POS_WEIGHT   ############
Used scale_pos_weight parameter with the suggested value = sum(negative instances) / sum(positive instances)

	********

The introduction of the parameter scale_pos_weight increased the recall performance, at the cost of lower accuracy.
However, we are still far from the goal.








		###### NEXT STEPS #######
- Basic model & data
- a bit of tuning with the three params:
	min_child_weight
	max_depth
	n_estimators -> you can go up to 100

- Unbalanced dataset: Use "imbalanced learn library"
	UNDERSAMPLER to perform undersampling -> might overfit the data, because it is generated from too little set
	SNOTE to perform oversampling -> might overfit the positive, as there will be a lot of "copies" in the training
- Decide what to do with the outliers:
	JOB: leave the category, or remove the rows
	EDUCATION: leave the category, make it the most frequent, or remove the rows
	CONTACT: leave the category
- Think what to do with outliers






To know which features influence the Y/N 
- partial dependent plot
- predicted vs observed -> like the frequency plot + scatter, but the scatter is both labels and prediction

Look if it's possible to group categories together.

Check feature importance: one way to do it is create a random feature and if a feature importance is lower than the random, just discard it
