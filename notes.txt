		###### MISSING DATA #######
There are three features with missing data (category 'unknown')
- JOB: a relatively small percentage of data (around 3%) 
- EDUCATION: a relatively small percentage of data (around 5%)
- CONTACT: a very high percentage of the data (around 40%)


		###### OUTLIER ANALYSIS #######
1/ Feature 'age'
The distribution is quite wide, with a small number of outliers. By zooming in the outlier region, we notice that the ratio between yes and no is much higher than in the center of the distribution.
This could indicate that samples that are outliers in this feature have an higher probability of saying yes. We will **not** remove the outliers from this feature

2/ Feature 'balance'
The distribution of the feature is very narrow, with outlier tails on both sides. 
The left outliers region is not particularly long, and it is mainly filled with 'no' samples -> we might remove the samples with balance lower than -1000. It also makes sense logically: a broke individual is not going to open a deposit.
The right region of outliers has two trends: in the first part (balance < 5000) we notice an increase of 'yes' samples, afterwards the outliers are mainly 'no'. 
In this case, we might keep only the samples with balance between -1000 and 5000

3/ Feature 'duration'
Once again the distribution is quite narrow, with a long outlier tail on the positive side.
Looking at the boxplot we notice that the distribution of 'yes' is centered around an higher value than the 'no'. This makes sense logically: if a customer accept an offer, the phone call will be longer - while if they are not interested, the call will be much shorted.
After the value 2000 (i.e. 33 minutes), however, the number of samples drastically reduces -> the samples in this area are not statistically meaningful, so we are going to remove them.

4/ Feature 'campaign'
Also in this case the distribution is narrow with a long positive tail.
There is a very limited number of 'yes' samples with campaign > 10, while the 'no's are much more. We can draw two conclusions from this:
a) we shall remove the outliers with campaign > 10
b) we shall advise our client to avoid calling the same individual more than 10 times


		###### CORRELATION ANALYSIS #######
1/ Categorical features (including 'day')
We observe some correlation between different categories and the probability of Yes. Hopefully, the model will identify and exploit such correlation
	JOB: the 'unknown' have an average probability of Yes - it could be a mix of the other categories
	EDUCATION: the 'unknown' have less than average probability of Yes, between the probability of class 'secondary' and 'primary' - could be a mix of the two categories
	CONTACT: the 'unknown' have a much less than average probability of Yes - we must keep the class as is

2/ Feature 'age'
We notice that in the center of the distribution, the probability of Yes is more or less constant -> we can consider to group together some values to reduce the number of possible values. 
In the outlier region, on the other hand, we observe and increase in the probability of Yes. However, after the value 80 the probability is very scattered -> the number of samples in this region is extremely low, we might consider to cut them off

3/ Feature 'campaign'
In the outlier analysis, we suggested to remove the samples with campaign > 10. Looking at the correlation, we notice that the probability of Yes drops after 10, with some peaks that are due to the very limited number of samples. 
In conclusion, the correlation analysis corroborates the conclusion of the outliers analysis

4/ Feature 'balance'
Once again, in the regions identified as outliers the probability of Yes is very low, with some peaks due to the low number of samples.
However, we notice that this behavior starts slightly after the region previously identified. Hence, we might consider 10000 instead of 5000 as upper bound.

5/ Feature 'duration'
We observe that the probability of Yes increases with the increase of the duration. However, once again, after some point the trend disrupts because of the limited number of samples.
Hence, also this analysis confirms that we shall remove the outliers with value above 2000


		############################################################
		#############		   XGBOOST		############
		############################################################
Training evaluation: train - test split 70-30 and metrics
	train confusion matrix
	test confusion matrix
	train accuracy
	test accuracy
	train recall
	test recall
	train AUC
	test AUC score

Final evaluation: mean accuracy with 5-fold cross validation
Find target categories with Partial Dependence Plot


		########## DEFAULT DATASET - DEFAULT PARAMS     ############
TRAINING EVALUATION
Train confusion matrix:
[[25876   108]
 [  529  1487]]
Test confusion matrix:
[[10864   256]
 [  550   330]]
Train accuracy score:  0.97725
Test accuracy score:  0.9328333333333333
Train recall score:  0.7375992063492064
Test recall score:  0.375
Train AUC score:  0.86672140120416
Test AUC score:  0.6759892086330935

FINAL EVALUATION
Train accuracy score:  0.977375
Test accuracy score:  0.6129999999999999
Train recall score:  0.7519877111654266
Test recall score:  0.19870585432672266

The train accuracy is higher than the test one -> we are overfitting
The recall is extremely low: this is due to the labels imbalance. 
Let's try to improve performance by using the parameter scale_pos_weight


		########## DEFAULT DATASET - SCALE_POS_WEIGHT   ############
Used scale_pos_weight parameter with the suggested value = sum(negative instances) / sum(positive instances) = 12.8

TRAIN EVALUATION
Train confusion matrix:
[[24449  1535]
 [    7  2009]]
Test confusion matrix:
[[10188   932]
 [  221   659]]
Train accuracy score:  0.9449285714285715
Test accuracy score:  0.9039166666666667
Train recall score:  0.9965277777777778
Test recall score:  0.7488636363636364
Train AUC score:  0.9687264812534209
Test AUC score:  0.8325253433616744

The introduction of the parameter scale_pos_weight increased the recall performance, at the cost of lower accuracy.
However, we are still far from the goal.


		########## DEFAULT DATASET - INITIAL TUNING   ############
1/ ITERATION 1
min_child_weight=10,
max_depth=4,
n_estimators=200,
scale_pos_weight=spw

Train confusion matrix:
[[23749  2235]
 [   29  1987]]
Test confusion matrix:
[[10035  1085]
 [  167   713]]
Train accuracy score:  0.9191428571428572
Test accuracy score:  0.8956666666666667
Train recall score:  0.9856150793650794
Test recall score:  0.8102272727272727
Train AUC score:  0.9498003044608648
Test AUC score:  0.8563276651406146


2/ ITERATION 2
min_child_weight=20,
max_depth=4,
n_estimators=400,
scale_pos_weight=spw

Train confusion matrix:
[[24055  1929]
 [    7  2009]]
Test confusion matrix:
[[10146   974]
 [  198   682]]
Train accuracy score:  0.9308571428571428
Test accuracy score:  0.9023333333333333
Train recall score:  0.9965277777777778
Test recall score:  0.775
Train AUC score:  0.9611448925834701
Test AUC score:  0.843705035971223


3/ ITERATION 3
min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=20

Train confusion matrix:
[[23656  2328]
 [    3  2013]]
Test confusion matrix:
[[9989 1131]
 [ 155  725]]
Train accuracy score:  0.91675
Test accuracy score:  0.8928333333333334
Train recall score:  0.9985119047619048
Test recall score:  0.8238636363636364
Train AUC score:  0.9544591543513956
Test AUC score:  0.8610775016350555


4/ ITERATION 4
min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=50

Train confusion matrix:
[[23012  2972]
 [    0  2016]]
Test confusion matrix:
[[9716 1404]
 [ 112  768]]
Train accuracy score:  0.8938571428571429
Test accuracy score:  0.8736666666666667
Train recall score:  1.0
Test recall score:  0.8727272727272727
Train AUC score:  0.942810960591133
Test AUC score:  0.8732341399607586

Train accuracy score:  0.9071999999999999
Test accuracy score:  0.5257499999999999
Train recall score:  1.0
Test recall score:  0.3150866535644095

Good improvement on the recall, however it overfits the train positive samples


5/ ITERATION 5
min_child_weight=10,
max_depth=4,
n_estimators=500,
scale_pos_weight=40

Train confusion matrix:
[[23931  2053]
 [    0  2016]]
Test confusion matrix:
[[10014  1106]
 [  183   697]]
Train accuracy score:  0.9266785714285715
Test accuracy score:  0.8925833333333333
Train recall score:  1.0
Test recall score:  0.7920454545454545
Train AUC score:  0.960494919950739
Test AUC score:  0.8462925114453892


6/ ITERATION 6
min_child_weight=15,
max_depth=4,
n_estimators=200,
scale_pos_weight=40

Train confusion matrix:
[[22587  3397]
 [    0  2016]]
Test confusion matrix:
[[9556 1564]
 [  95  785]]
Train accuracy score:  0.8786785714285714
Test accuracy score:  0.86175
Train recall score:  1.0
Test recall score:  0.8920454545454546
Train AUC score:  0.9346328509852216
Test AUC score:  0.8756989862655331

Good predictions on the positive, but we lost slightly too much in the negative

7/ ITERATION 7
min_child_weight=15,
max_depth=5,
n_estimators=300,
scale_pos_weight=40

Train confusion matrix:
[[23798  2186]
 [    0  2016]]
Test confusion matrix:
[[9977 1143]
 [ 166  714]]
Train accuracy score:  0.9219285714285714
Test accuracy score:  0.8909166666666667
Train recall score:  1.0
Test recall score:  0.8113636363636364
Train AUC score:  0.9579356527093595
Test AUC score:  0.8542879332897318

8/ ITERATION 8
min_child_weight=15,
max_depth=5,
n_estimators=200,
scale_pos_weight=40


Train confusion matrix:
[[23292  2692]
 [    0  2016]]
Test confusion matrix:
[[9784 1336]
 [ 130  750]]
Train accuracy score:  0.9038571428571428
Test accuracy score:  0.8778333333333334
Train recall score:  1.0
Test recall score:  0.8522727272727273
Train AUC score:  0.9481988916256158
Test AUC score:  0.8660644211903203

9/ITERATION 9
min_child_weight=10,
max_depth=5,
n_estimators=200,
scale_pos_weight=40

Train confusion matrix:
[[23450  2534]
 [    0  2016]]
Test confusion matrix:
[[9862 1258]
 [ 134  746]]
Train accuracy score:  0.9095
Test accuracy score:  0.884
Train recall score:  1.0
Test recall score:  0.8477272727272728
Train AUC score:  0.951239224137931
Test AUC score:  0.8672988881621975


10/ ITERATION 10
min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=40

Train confusion matrix:
[[23058  2926]
 [    0  2016]]
Test confusion matrix:
[[9735 1385]
 [ 123  757]]
Train accuracy score:  0.8955
Test accuracy score:  0.8743333333333333
Train recall score:  1.0
Test recall score:  0.8602272727272727
Train AUC score:  0.9436961206896552
Test AUC score:  0.8678384565075213

11/ ITERATION 11	
Add the learning_rate as tuned parameter as well:
min_child_weight=15,
max_depth=4,
n_estimators=500,
learning_rate=0.1,
scale_pos_weight=50

Train confusion matrix:
[[21992  3992]
 [    2  2014]]
Test confusion matrix:
[[9338 1782]
 [  61  819]]
Train accuracy score:  0.8573571428571428
Test accuracy score:  0.8464166666666667
Train recall score:  0.9990079365079365
Test recall score:  0.9306818181818182
Train AUC score:  0.9226874657909141
Test AUC score:  0.8852150098103335

We improved a lot on the positive results, but we are slightly underfitting -> increase the number of trees

12/ ITERATION 12
min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=50

Train confusion matrix:
[[22948  3036]
 [    0  2016]]
Test confusion matrix:
[[9686 1434]
 [ 102  778]]
Train accuracy score:  0.8915714285714286
Test accuracy score:  0.872
Train recall score:  1.0
Test recall score:  0.884090909090909
Train AUC score:  0.9415794334975369
Test AUC score:  0.8775670372792674

Train accuracy score:  0.907175
Test accuracy score:  0.490925
Train recall score:  1.0
Test recall score:  0.3147257459353225



CONCLUSIONS -> ITERATIONS 4 AND 12 ARE MORE OR LESS EQUIVALENT
However, iteration 12 has more trees, which suits better for early stopping -> we fix the parameters to iteration 12
min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=50,
random_state=42


		########## UNDERSAMPLED DATASET - ITERATION 4 AND 12 TUNING  ############
Undersampling with strategy 1.0

min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=50

Train confusion matrix:
[[1535  481]
 [   0 2016]]
Test confusion matrix:
[[8017 3103]
 [  21  859]]
Train accuracy score:  0.8807043650793651
Test accuracy score:  0.7396666666666667
Train recall score:  1.0
Test recall score:  0.9761363636363637
Train AUC score:  0.880704365079365
Test AUC score:  0.8485448005232178

Now the training set is balanced, hence we should reduce the pos_weight. 

min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=spw

Train confusion matrix:
[[1645  371]
 [   0 2016]]
Test confusion matrix:
[[8420 2700]
 [  27  853]]
Train accuracy score:  0.9079861111111112
Test accuracy score:  0.77275
Train recall score:  1.0
Test recall score:  0.9693181818181819
Train AUC score:  0.9079861111111112
Test AUC score:  0.8632562132112491

Better fit (accuracy) without loosing a lot in the recall -> good!
To increase the test accuracy, we could reduce a bit the ratio of the undersampler -> more examples of Negative samples for the model to train on
Strategy 0.3

min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=spw

Train confusion matrix:
[[5826  894]
 [   0 2016]]
Test confusion matrix:
[[9342 1778]
 [  81  799]]
Train accuracy score:  0.8976648351648352
Test accuracy score:  0.8450833333333333
Train recall score:  1.0
Test recall score:  0.9079545454545455
Train AUC score:  0.9334821428571429
Test AUC score:  0.8740312295618051

Good result, but ITERATION 4 is still better in term of accuracy -> try to further lower the undersampling ratio
strategy 0.2

min_child_weight=15,
max_depth=4,
n_estimators=300,
scale_pos_weight=spw

Train confusion matrix:
[[8955 1125]
 [   1 2015]]
Test confusion matrix:
[[9670 1450]
 [ 101  779]]
Train accuracy score:  0.9069113756613757
Test accuracy score:  0.87075
Train recall score:  0.9995039682539683
Test recall score:  0.8852272727272728
Train AUC score:  0.9439484126984127
Test AUC score:  0.8774157946370177

LET'S NOW TRY OUT WITH ITERATION 12 TUNING
min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=50

Train confusion matrix:
[[8501 1579]
 [   0 2016]]
Test confusion matrix:
[[9229 1891]
 [  56  824]]
Train accuracy score:  0.8694609788359788
Test accuracy score:  0.83775
Train recall score:  1.0
Test recall score:  0.9363636363636364
Train AUC score:  0.9216765873015873
Test AUC score:  0.883154839764552

Good resutls, almost comparable to the ones above.
But this time we are weighting a lot the postive samples -> reduce it

min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=spw

Train confusion matrix:
[[8948 1132]
 [   2 2014]]
Test confusion matrix:
[[9646 1474]
 [  98  782]]
Train accuracy score:  0.90625
Test accuracy score:  0.869
Train recall score:  0.9990079365079365
Test recall score:  0.8886363636363637
Train AUC score:  0.9433531746031746
Test AUC score:  0.8780412034009156

Train accuracy score:  0.907175
Test accuracy score:  0.490925
Train recall score:  1.0
Test recall score:  0.3147257459353225

Performance very similar to iteration 4.

Conclusion -> using a more balanced training set increase performance (if we reduce the scale pos weight)
However, we observe an underfitting due to the small portion of the used training set


		########## OVERSAMPLED DATASET - ITERATION 4 AND 12 TUNING  ############
strategy 1.0
The training set is much bigger than in the undersampled case -> a lot of copies of the positive samples
The model should highly overfit the positive training samples

min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=spw

Train confusion matrix:
[[23896  2088]
 [   17 25967]]
Test confusion matrix:
[[10081  1039]
 [  180   700]]
Train accuracy score:  0.9594943041871922
Test accuracy score:  0.8984166666666666
Train recall score:  0.9993457512315271
Test recall score:  0.7954545454545454
Train AUC score:  0.9594943041871921
Test AUC score:  0.8510096468279921


min_child_weight=15,
max_depth=4,
n_estimators=300,
learning_rate=0.3,
scale_pos_weight=spw

Train confusion matrix:
[[23916  2068]
 [   12 25972]]
Test confusion matrix:
[[10074  1046]
 [  182   698]]
Train accuracy score:  0.9599753694581281
Test accuracy score:  0.8976666666666666
Train recall score:  0.9995381773399015
Test recall score:  0.7931818181818182
Train AUC score:  0.9599753694581281
Test AUC score:  0.8495585349901896

We are overfitting the positive train samples, because a lot of copies are present in the training set.
Once again, by reducing the sampling strategy we shuold improve the performance

strategy 0.2

min_child_weight=15,
max_depth=4,
n_estimators=1000,
learning_rate=0.1,
scale_pos_weight=spw

Train confusion matrix:
[[23935  2049]
 [   13  5183]]
Test confusion matrix:
[[10091  1029]
 [  180   700]]
Train accuracy score:  0.9338678640153945
Test accuracy score:  0.89925
Train recall score:  0.9974980754426482
Test recall score:  0.7954545454545454
Train AUC score:  0.9593209281154129
Test AUC score:  0.8514592871157619


min_child_weight=15,
max_depth=4,
n_estimators=300,
learning_rate=0.3,
scale_pos_weight=spw

Train confusion matrix:
[[23915  2069]
 [   13  5183]]
Test confusion matrix:
[[10106  1014]
 [  177   703]]
Train accuracy score:  0.9332264271969211
Test accuracy score:  0.90075
Train recall score:  0.9974980754426482
Test recall score:  0.7988636363636363
Train AUC score:  0.958936075898664
Test AUC score:  0.8538382930019621

Not a significant improve. 

CONCLUSIONS 
Undersampling: training set size 28'000 -> 12'000
Oversampling: training set size 28'000 -> 31'000

Since we are more interested in the Positive samples, which are the minority class, undersampling is more advisable:
- The number of "original" positive samples is always the same
- But the number of negative samples the model trains on is significantly less 
Hence, the model will learn better the Positive samples, without using too many resources on the negative ones 

HOWEVER
Under and Over sampling are based on the assumption that the sampled data do not represent the real data distribution.
From the problem description, however, we deduce that the strong unbalance is expected.
Hence, under and over sampling are not the recommended way to proceed




		###### NEXT STEPS #######
- Unbalanced dataset: Use "imbalanced learn library"
	UNDERSAMPLER to perform undersampling -> might overfit the data, because it is generated from too little set
	SNOTE to perform oversampling -> might overfit the positive, as there will be a lot of "copies" in the training
- Decide what to do with the missing values:
	JOB: leave the category, or remove the rows
	EDUCATION: leave the category, make it the most frequent, or remove the rows
	CONTACT: leave the category
- Think what to do with outliers






To know which features influence the Y/N 
- partial dependent plot
- predicted vs observed -> like the frequency plot + scatter, but the scatter is both labels and prediction

Look if it's possible to group categories together.

Check feature importance: one way to do it is create a random feature and if a feature importance is lower than the random, just discard it
